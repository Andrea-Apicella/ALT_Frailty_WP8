{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating CNN+RNN models on the dataset\n",
    "\n",
    "# Imports\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from statistics import mode\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import wandb\n",
    "from wp8.options.train_options import TrainOptions\n",
    "from wp8.pre_processing.generators import TimeSeriesGenerator as TSG\n",
    "from wp8.pre_processing.utils import safe_mkdir\n",
    "from wp8.utils.cnn_rnn_utils import get_timeseries_labels_encoded, load_and_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nested works too'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "mydict = {'val':'it works'}\n",
    "nested_dict = {'val':'nested works too'}\n",
    "mydict = dotdict(mydict)\n",
    "mydict.val\n",
    "# 'it works'\n",
    "\n",
    "mydict.nested = dotdict(nested_dict)\n",
    "mydict.nested.val\n",
    "# 'nested works too'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = TrainOptions().parse()\n",
    "opt = dotdict({\n",
    "  \"lstm1_units\": 256,\n",
    "  \"lstm2_units\": 128,\n",
    "  \"dropout\": 0.3,\n",
    "  \"epochs\": 50,\n",
    "  \"train_actors\": [1, 2, 3],\n",
    "  \"val_actors\": [4],\n",
    "  \"train_cams\": [1, 2],\n",
    "  \"val_cams\": [1],\n",
    "  \"seq_len\": 20,\n",
    "  \"split_ratio\": None,\n",
    "  \"drop_offair\": False,\n",
    "  \"undersample\": False,\n",
    "  \"batch_size\": 60,\n",
    "  \"stride\": 10,\n",
    "  \"learning_rate\": 1e-5,\n",
    "})\n",
    "\n",
    "# if set(opt.train_actors) & set(opt.val_actors):\n",
    "#     raise Exception(\"Can't use the same actors both in train and validation splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB project initialization\n",
    "run = wandb.init(\n",
    "    project=\"Fall detection CNN + RNN\",\n",
    "    config={\n",
    "        \"model\": \"LSTM\",\n",
    "        \"epochs\": opt.epochs,\n",
    "        \"seq_len\": opt.seq_len,\n",
    "        \"num_features\": 2048,\n",
    "        \"batch_size\": opt.batch_size,\n",
    "        \"stride\": opt.stride,\n",
    "        \"loss_function\": \"sparse_categorical_crossentropy\",\n",
    "        \"architecture\": \"LSTM\",\n",
    "        \"train_actors\": opt.train_actors,\n",
    "        \"val_actors\": opt.val_actors,\n",
    "        \"train_cams\": opt.train_cams,\n",
    "        \"val_cams\": opt.val_cams,\n",
    "        \"dropout\": opt.dropout,\n",
    "        \"lstm1_units\": opt.lstm1_units,\n",
    "        \"lstm2_units\": opt.lstm2_units,\n",
    "        \"learning_rate\": opt.learning_rate,\n",
    "        \"split_ratio\": opt.split_ratio,\n",
    "        \"drop_offair\": opt.drop_offair,\n",
    "        \"undersample\": opt.undersample,\n",
    "    },\n",
    ")\n",
    "\n",
    "cfg = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] Load Train Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading features: outputs/dataset/features/Actor_3_Walk_Stick_Full_PH.npz: 100%|██████████| 44/44 [00:43<00:00,  1.02it/s]\n",
      "Loading csv datasets: 100%|██████████| 44/44 [00:01<00:00, 30.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] Load Val Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading features: outputs/dataset/features/Actor_4_Chair_Full_PH.npz: 100%|██████████| 6/6 [00:07<00:00,  1.22s/it]   \n",
      "Loading csv datasets: 100%|██████████| 6/6 [00:00<00:00, 33.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train shape: (299400, 2048), len y_train: 299400, X_val shape: (24060, 2048), len y_val: 24060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, cams_train, cams_val = load_and_split(opt.train_actors, opt.val_actors, opt.train_cams, opt.val_cams, opt.split_ratio, opt.drop_offair, opt.undersample)\n",
    "print(f\"\\nX_train shape: {X_train.shape}, len y_train: {len(y_train)}, X_val shape: {X_val.shape}, len y_val: {len(y_val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights for train series: {'crouched_still': 19.192307692307693, 'fall_crouch': 3.9167974882260594, 'fall_frontal': 4.361888111888112, 'fall_lateral': 2.741758241758242, 'lie_down_from_sitting': 0.6953734671125975, 'lie_down_on_the_floor': 2.020242914979757, 'lie_still': 0.1894600956792467, 'sit_down_from_standing': 1.6545092838196287, 'sit_still': 6.397435897435898, 'sit_up_from_lying': 0.783359497645212, 'stand_still': 2.9079254079254078, 'stand_up_from_floor': 0.5515030946065429, 'stand_up_from_sit': 1.4430306535569692}\n"
     ]
    }
   ],
   "source": [
    "y_train_series, y_val_series, enc, class_weights = get_timeseries_labels_encoded(y_train, y_val, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2495, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train_series.shape)\n",
    "np.unique(y_train_series, axis=0).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = TSG(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    num_features=cfg.num_features,\n",
    "    cams=cams_train,\n",
    "    batch_size=cfg.batch_size,\n",
    "    stride=cfg.stride,\n",
    "    seq_len=cfg.seq_len,\n",
    "    labels_encoder=enc,\n",
    ")\n",
    "val_gen = TSG(\n",
    "    X=X_val,\n",
    "    y=y_val,\n",
    "    cams=cams_val,\n",
    "    num_features=cfg.num_features,\n",
    "    batch_size=cfg.batch_size,\n",
    "    stride=cfg.stride,\n",
    "    seq_len=cfg.seq_len,\n",
    "    labels_encoder=enc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 10:35:23.906318: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-23 10:35:23.907849: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20, 256)           2360320   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 256)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,559,117\n",
      "Trainable params: 2,559,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=cfg.lstm1_units, input_shape=(cfg.seq_len, cfg.num_features), return_sequences=True))\n",
    "model.add(Dropout(cfg.dropout))\n",
    "model.add(LSTM(units=cfg.lstm2_units, input_shape=(cfg.seq_len, cfg.num_features)))\n",
    "model.add(Dropout(cfg.dropout))\n",
    "model.add(Dense(np.unique(y_train_series, axis=0).shape[0], activation=\"softmax\"))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    loss=cfg.loss_function,\n",
    "    metrics=[\"accuracy\", \"categorical_crossentropy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "dir_path = f\"model_checkpoints/{cfg.model}\"\n",
    "safe_mkdir(dir_path)\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"{dir_path}/{cfg.model}_{dt_string}\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    initial_value_threshold=0.8,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callbacks = [WandbCallback(), model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfit(train_gen, validation_data\u001b[38;5;241m=\u001b[39mval_gen, epochs\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mepochs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, class_weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[1;32m      3\u001b[0m val_gen\u001b[38;5;241m.\u001b[39mevaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=cfg.epochs, callbacks=callbacks, class_weight=class_weights)\n",
    "val_gen.evaluate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "val_logits = model.predict(val_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X_train\n",
    "del y_train\n",
    "del X_val\n",
    "del y_val\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics to wandb\n",
    "y_pred_val_classes = np.argmax(val_logits, axis=1).tolist()\n",
    "\n",
    "# wandb.sklearn.plot_roc(y_val_series, val_logits, classes)\n",
    "# wandb.sklearn.plot_class_proportions(y_train_series, y_val_series, classes)\n",
    "# wandb.sklearn.plot_precision_recall(y_val_series, val_logits, classes)\n",
    "# wandb.sklearn.plot_confusion_matrix(y_val_series, y_pred_val_classes, classes)\n",
    "wandb.join()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "045da2998f02dde6c755b0fa2e74d8766d7d056d6163358ab445ad432d6df67f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
