{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating CNN+RNN models on the dataset\n",
    "\n",
    "# Imports\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from statistics import mode\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import wandb\n",
    "from wp8.options.train_options import TrainOptions\n",
    "from wp8.pre_processing.generators import TimeSeriesGenerator as TSG\n",
    "from wp8.pre_processing.utils import safe_mkdir\n",
    "from wp8.utils.cnn_rnn_utils import get_timeseries_labels_encoded, load_and_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "mydict = {'val':'it works'}\n",
    "nested_dict = {'val':'nested works too'}\n",
    "mydict = dotdict(mydict)\n",
    "mydict.val\n",
    "# 'it works'\n",
    "\n",
    "mydict.nested = dotdict(nested_dict)\n",
    "mydict.nested.val\n",
    "# 'nested works too'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = TrainOptions().parse()\n",
    "opt = dotdict({\n",
    "  \"lstm1_units\": 256,\n",
    "  \"lstm2_units\": 128,\n",
    "  \"dropout\": 0.3,\n",
    "  \"epochs\": 50,\n",
    "  \"train_actors\": [1, 2,3],\n",
    "  \"val_actors\": [4],\n",
    "  \"train_cams\": [1, 2],\n",
    "  \"val_cams\": [1],\n",
    "  \"seq_len\": 20,\n",
    "  \"split_ratio\": None,\n",
    "  \"drop_offair\": False,\n",
    "  \"undersample\": False,\n",
    "  \"batch_size\": 60,\n",
    "  \"stride\": 10,\n",
    "  \"learning_rate\": 1e-5,\n",
    "})\n",
    "\n",
    "# if set(opt.train_actors) & set(opt.val_actors):\n",
    "#     raise Exception(\"Can't use the same actors both in train and validation splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB project initialization\n",
    "run = wandb.init(\n",
    "    project=\"Fall detection CNN + RNN\",\n",
    "    config={\n",
    "        \"model\": \"LSTM\",\n",
    "        \"epochs\": opt.epochs,\n",
    "        \"seq_len\": opt.seq_len,\n",
    "        \"num_features\": 2048,\n",
    "        \"batch_size\": opt.batch_size,\n",
    "        \"stride\": opt.stride,\n",
    "        \"loss_function\": \"sparse_categorical_crossentropy\",\n",
    "        \"architecture\": \"LSTM\",\n",
    "        \"train_actors\": opt.train_actors,\n",
    "        \"val_actors\": opt.val_actors,\n",
    "        \"train_cams\": opt.train_cams,\n",
    "        \"val_cams\": opt.val_cams,\n",
    "        \"dropout\": opt.dropout,\n",
    "        \"lstm1_units\": opt.lstm1_units,\n",
    "        \"lstm2_units\": opt.lstm2_units,\n",
    "        \"learning_rate\": opt.learning_rate,\n",
    "        \"split_ratio\": opt.split_ratio,\n",
    "        \"drop_offair\": opt.drop_offair,\n",
    "        \"undersample\": opt.undersample,\n",
    "    },\n",
    ")\n",
    "\n",
    "cfg = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, cams_train, cams_val = load_and_split(opt.train_actors, opt.val_actors, opt.train_cams, opt.val_cams, opt.split_ratio, opt.drop_offair, opt.undersample)\n",
    "print(f\"\\nX_train shape: {X_train.shape}, len y_train: {len(y_train)}, X_val shape: {X_val.shape}, len y_val: {len(y_val)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_series, y_val_series, enc, class_weights, classes = get_timeseries_labels_encoded(y_train, y_val, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_series.shape)\n",
    "np.unique(y_train_series, axis=0).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = TSG(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    num_features=cfg.num_features,\n",
    "    cams=cams_train,\n",
    "    batch_size=cfg.batch_size,\n",
    "    stride=cfg.stride,\n",
    "    seq_len=cfg.seq_len,\n",
    "    labels_encoder=enc,\n",
    ")\n",
    "val_gen = TSG(\n",
    "    X=X_val,\n",
    "    y=y_val,\n",
    "    cams=cams_val,\n",
    "    num_features=cfg.num_features,\n",
    "    batch_size=cfg.batch_size,\n",
    "    stride=cfg.stride,\n",
    "    seq_len=cfg.seq_len,\n",
    "    labels_encoder=enc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=cfg.lstm1_units, input_shape=(cfg.seq_len, cfg.num_features), return_sequences=True))\n",
    "model.add(Dropout(cfg.dropout))\n",
    "model.add(LSTM(units=cfg.lstm2_units, input_shape=(cfg.seq_len, cfg.num_features)))\n",
    "model.add(Dropout(cfg.dropout))\n",
    "model.add(Dense(np.unique(y_train_series, axis=0).shape[0], activation=\"softmax\"))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    loss=cfg.loss_function,\n",
    "    metrics=[\"accuracy\", \"sparse_categorical_crossentropy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "dir_path = f\"model_checkpoints/{cfg.model}\"\n",
    "safe_mkdir(dir_path)\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"{dir_path}/{cfg.model}_{dt_string}\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    initial_value_threshold=0.8,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callbacks = [WandbCallback(), model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=cfg.epochs, callbacks=callbacks, class_weight=class_weights)\n",
    "val_gen.evaluate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "val_logits = model.predict(val_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X_train\n",
    "del y_train\n",
    "del X_val\n",
    "del y_val\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics to wandb\n",
    "y_pred_val_classes = np.argmax(val_logits, axis=1).tolist()\n",
    "\n",
    "wandb.sklearn.plot_roc(y_val_series, val_logits, classes)\n",
    "wandb.sklearn.plot_class_proportions(y_train_series, y_val_series, classes)\n",
    "wandb.sklearn.plot_precision_recall(y_val_series, val_logits, classes)\n",
    "wandb.sklearn.plot_confusion_matrix(y_val_series, y_pred_val_classes, classes)\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "045da2998f02dde6c755b0fa2e74d8766d7d056d6163358ab445ad432d6df67f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
